{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021, Battelle Energy Alliance, LLC\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "from statistics import mode\n",
    "\n",
    "\n",
    "# Change working directory if not the project directory\n",
    "current_dir = os.getcwd()\n",
    "folders = re.split('\\/', current_dir)\n",
    "if folders[len(folders)-1] == 'split':\n",
    "    os.chdir(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "# Load environment variables from .env file    \n",
    "!pip install python-dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "import settings\n",
    "#%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.getenv(\"ML_ADAPTER_OBJECT_LOCATION\"), 'r') as fp:\n",
    "    data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProtoTuple = namedtuple('ProtoTuple', ['proto_dist', 'proto_index'])\n",
    "CompareClusterProto = namedtuple('CompareClusterProto', ['min_index', 'proto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_matrix(matrix: np.ndarray):\n",
    "    \"\"\"\n",
    "    This function determines the euclidean distances of a matrix\n",
    "    \"\"\"\n",
    "    from scipy import spatial\n",
    "    dist_vector = spatial.distance.pdist(matrix, 'euclidean')\n",
    "    square_distance_matrix = spatial.distance.squareform(dist_vector)\n",
    "    return square_distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cluster():\n",
    "    \"\"\"\n",
    "    This class contains all of the information for a given cluster.\n",
    "\n",
    "    Args:\n",
    "        id (str): The unique identifier of the cluster.  As clusters are merged, the index increases in size.\n",
    "        previous_link_id (list): This list contains the pair of previous clusters ids that were merged. When empty, there is no prior cluster (single point of information).\n",
    "        next_link_id (list): This list contains the next cluster (filled in after applying hierarchical clustering).\n",
    "        proto (ProtoTuple): This includes the proto_dist and proto_index.\n",
    "        sample_indices (int list): A list of ints that describe which sample indicies are contained within this cluster.\n",
    "    \"\"\"\n",
    "    # using slots to save time and memory\n",
    "    __slots__ = ('id', 'previous_id', 'next_id', 'proto', 'sample_indices')\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.id = 1\n",
    "        self.previous_id = []\n",
    "        self.next_id = -1\n",
    "        self.proto = None\n",
    "        self.sample_indices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimax_dist(dist_matrix: np.ndarray, cluster_index: np.ndarray) -> ProtoTuple:\n",
    "    \"\"\"\n",
    "    This function is used to determine the minimax distance between two different clusters.\n",
    "    \n",
    "    Args:\n",
    "        dist_matrix (ndarray): This is an numpy array containing the distances between all samples.\n",
    "        cluster_index (ndarray): A set of indices within the distance matrix.\n",
    "    \n",
    "    Return:\n",
    "        minimax_dist (ProtoTuple): The minimax radius and prototype index as a list.\n",
    "    \"\"\"\n",
    "    # create a matrix that is just the current samples\n",
    "    sub_matrix = dist_matrix[cluster_index[:, None], cluster_index]\n",
    "    # grab the sum per row\n",
    "    sum_per_row = np.apply_along_axis(np.nansum, 0, sub_matrix)\n",
    "    # determine the sample that is closest to all other samples\n",
    "    min_index = np.nanargmin(sum_per_row)\n",
    "    # find the radius determined by the maximum from the min centriod\n",
    "    proto_dist = np.nanmax(sub_matrix[min_index])\n",
    "    # since a sub_matrix was created, determine the proper index\n",
    "    proto_index = cluster_index[min_index]\n",
    "    minimax_dist = ProtoTuple(proto_dist, proto_index)\n",
    "\n",
    "    return minimax_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_clusters(dist_matrix:np.ndarray, cluster_list:list, top_of_stack:Cluster) -> CompareClusterProto:\n",
    "    \"\"\"\n",
    "    This function is used to loop through each cluster to determine the cluster that is the closest cluster to the top of the stack.\n",
    "\n",
    "    Args:\n",
    "        dist_matrix (ndarray): This is an numpy array containing the distances between all samples.\n",
    "        cluster_list (list): A list of cluster objects to compare tothe top of the stack in the nearest neighbor function.\n",
    "        top_of_stack (Cluster): The current top of the stack for the nearest neighbor function.\n",
    "    \n",
    "    Return:\n",
    "        CompareClusterProto (CompareClusterProto): The index of the closest cluster and prototype associated with the combination of the closest cluster and the top of the stack.\n",
    "\n",
    "    \"\"\"\n",
    "    proto_dist = None\n",
    "    min_index = None\n",
    "    proto_index = None\n",
    "    # looping over all clusters\n",
    "    for i in cluster_list:\n",
    "        # if the value of i is the top of the stack, then skip\n",
    "        if top_of_stack.id == i.id:\n",
    "            continue\n",
    "\n",
    "        # getting the appropriate row indices for the top of the stack and the iterator i\n",
    "        cluster_index = np.array(top_of_stack.sample_indices + i.sample_indices)\n",
    "        single_dist = minimax_dist(dist_matrix, cluster_index)\n",
    "        # The if portion initializes the proto dist and index based on the first value of i\n",
    "        # The else portion checks to see if any iterations of i are closer to the top of the stack and replaces the proto index if closer to the top of the stack\n",
    "        if proto_dist is None:\n",
    "            proto_dist = single_dist.proto_dist\n",
    "            proto_index = single_dist.proto_index\n",
    "            min_index = i.id\n",
    "        else:\n",
    "            if proto_dist > single_dist.proto_dist:\n",
    "                proto_dist = single_dist.proto_dist\n",
    "                proto_index = single_dist.proto_index\n",
    "                min_index = i.id\n",
    "\n",
    "    return CompareClusterProto(min_index, ProtoTuple(proto_dist, proto_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_active_clusters(dist_matrix):\n",
    "    \"\"\"\n",
    "        This function takes a distance matrix and creates an object\n",
    "    \"\"\"\n",
    "    active_clusters = []\n",
    "    for i in range(dist_matrix.shape[1]):\n",
    "        new_obj = Cluster()\n",
    "        current_sample = int(i)\n",
    "        new_obj.id = current_sample\n",
    "        new_obj.proto = ProtoTuple(0, current_sample)\n",
    "        new_obj.sample_indices.append(current_sample)\n",
    "        active_clusters.append(new_obj)\n",
    "\n",
    "    return active_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor(dist_matrix: np.ndarray):\n",
    "    \"\"\"\n",
    "\n",
    "    This function is used to determine the minimax distance between two different clusters.\n",
    "\n",
    "    From Wikipedia:\n",
    "    Initialize the set of active clusters to consist of n one-point clusters, one for each input point.\n",
    "    Let S be a stack data structure, initially empty, the elements of which will be active clusters.\n",
    "    While there is more than one cluster in the set of clusters:\n",
    "        If S is empty, choose an active cluster arbitrarily and push it onto S.\n",
    "        Let C be the active cluster on the top of S. Compute the distances from C to all other clusters, and let D be the nearest other cluster.\n",
    "        If D is already in S, it must be the immediate predecessor of C. Pop both clusters from S and merge them.\n",
    "        Otherwise, if D is not already in S, push it onto S.\n",
    "\n",
    "    Args:\n",
    "        dist_matrix (ndarray): This is an numpy array containing the distances between all samples.\n",
    "\n",
    "    \n",
    "    Return:\n",
    "        retired_clusters (Cluster list): A list of all of the clusters obtained by prototypical clustering.\n",
    "\n",
    "    \"\"\"\n",
    "    # dist_matrix should be the distance within each different active cluster\n",
    "    # create a set of active clusters for each sample in the dist_matrix\n",
    "    # active_clusters is a list of Cluster\n",
    "    active_clusters = create_active_clusters(dist_matrix)\n",
    "    stack = []\n",
    "    retired_clusters = []\n",
    "    len_active_clusters = len(active_clusters)\n",
    "    num_clusters = len(active_clusters)\n",
    "    while len_active_clusters > 1:\n",
    "        # grab the last active cluster\n",
    "        if len(stack) == 0:\n",
    "            stack.append(active_clusters[0])\n",
    "\n",
    "        stack_ids = [i.id for i in stack]\n",
    "        active_ids = [i.id for i in active_clusters]\n",
    "        closest_cluster = compare_clusters(dist_matrix, active_clusters, stack[-1])\n",
    "        chosen_cluster = active_clusters[int(np.where(np.array(active_ids) == closest_cluster.min_index)[0])]\n",
    "        if chosen_cluster.id in stack_ids:\n",
    "            # merge closest and last stack\n",
    "            num_clusters += 1\n",
    "            new_cluster = Cluster()\n",
    "            new_cluster.id = num_clusters\n",
    "            new_cluster.previous_id = [chosen_cluster.id, stack[-1].id]\n",
    "            new_cluster.proto = closest_cluster.proto\n",
    "            new_cluster.sample_indices = chosen_cluster.sample_indices + stack[-1].sample_indices\n",
    "            # remove clusters from active_clusters and add merged\n",
    "            active_clusters = [i for i in active_clusters if i.id not in new_cluster.previous_id]\n",
    "            active_clusters.append(new_cluster)\n",
    "            # remove clusters from stack\n",
    "            stack[-1].next_id = num_clusters\n",
    "            stack[-2].next_id = num_clusters\n",
    "            retired_clusters.append(stack.pop())\n",
    "            retired_clusters.append(stack.pop())\n",
    "            len_active_clusters -= 1\n",
    "        else:\n",
    "            stack.append(chosen_cluster)\n",
    "\n",
    "    retired_clusters.append(active_clusters[0])\n",
    "    return retired_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dendrogram(retired_clusters):\n",
    "    \"\"\"\n",
    "    Plots a dendrogram from the hierarchical clustering algorithm\n",
    "    \n",
    "    Args:\n",
    "        retired_clusters (Cluster list): A list of all of the clusters obtained by prototypical clustering.\n",
    "    \"\"\"\n",
    "    import networkx as nx\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    G = nx.Graph()\n",
    "    for clust in retired_clusters:\n",
    "        G.add_node(clust.id)\n",
    "    for clust in retired_clusters:\n",
    "        if clust.previous_id:\n",
    "            for prev in clust.previous_id:\n",
    "                G.add_edge(prev, clust.id)\n",
    "        if clust.next_id != -1:\n",
    "            G.add_edge(clust.next_id, clust.id)\n",
    "    ax1 = plt.subplot()\n",
    "    nx.draw(G, with_labels=True, font_weight='bold')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(retired_clusters: list, k_clusters: int):\n",
    "    \"\"\"\n",
    "    This function cuts the tree to a specified number of clusters\n",
    "    \n",
    "    Args:\n",
    "        retired_clusters (Cluster list): A list of all of the clusters obtained by prototypical clustering.\n",
    "        k_clusters (int): the number of clusters desired\n",
    "    \n",
    "    Return:\n",
    "        clusters (Cluster list): A list of chosen clusters after cutting of the tree \n",
    "    \"\"\"\n",
    "    clusters = list()\n",
    "    cluster_options = list()\n",
    "    while len(clusters) < k_clusters:\n",
    "        # Delete last cluster in retired clusters\n",
    "        retired_clust = retired_clusters.pop()\n",
    "        # Add previous ids clusters to cluster options list\n",
    "        for i in range(len(retired_clust.previous_id)):\n",
    "            for j in reversed(range(len(retired_clusters))):\n",
    "                if retired_clusters[j].id == retired_clust.previous_id[i]:\n",
    "                    cluster_options.append(retired_clusters[j])\n",
    "                    break\n",
    "\n",
    "        # Pick the cluster with the max distance\n",
    "        max_dist = None\n",
    "        max_index = None\n",
    "        for i in range(len(cluster_options)):\n",
    "            dist = cluster_options[i].proto[0]\n",
    "            # Initialize max_dist to first cluster's distance\n",
    "            if max_dist == None:\n",
    "                max_dist = dist\n",
    "                max_index = i\n",
    "            # Update max_dist when new distance is greater\n",
    "            elif max_dist < dist:\n",
    "                max_dist = dist\n",
    "                max_index = i\n",
    "        # Add max distance cluster to clusters list\n",
    "        clusters.append(cluster_options[max_index])\n",
    "        cluster_options.pop(max_index)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_clusters(clusters: list):\n",
    "    \"\"\"\n",
    "    This function prunes clusters to contain unique sample indices for every cluster (no repeats)\n",
    "    \n",
    "    Args:\n",
    "        clusters (Cluster list): A list of chosen clusters after cutting of the tree \n",
    "    \n",
    "    Return:\n",
    "        pruned_clusters (Cluster list): A list of pruned clusters where each cluster has a unique set of sample indices\n",
    "        prototype_ids (integer list): A list of row indices of the prototype (center point in a cluster)\n",
    "    \"\"\"\n",
    "    pruned_clusters = list()\n",
    "    # Order the clusters by id\n",
    "    cluster_ids = dict()\n",
    "    prototype_ids = list()\n",
    "    count = 0\n",
    "    for i in clusters:\n",
    "        cluster_ids[count] = i.id\n",
    "        count += 1\n",
    "    cluster_ids = dict(sorted(cluster_ids.items(), key=lambda item: item[1]))\n",
    "    ordered_clusters = list()\n",
    "    for index in cluster_ids.keys():\n",
    "        ordered_clusters.append(clusters[index])\n",
    "\n",
    "    assigned_indices = list()\n",
    "    for clust in ordered_clusters:\n",
    "        # Create a pruned sample indices list filtering out repeated indices\n",
    "        sample_indices = list()\n",
    "        for index in clust.sample_indices:\n",
    "            if index not in assigned_indices:\n",
    "                sample_indices.append(index)\n",
    "        if sample_indices:\n",
    "            # Add indices to assigned indices list\n",
    "            assigned_indices.extend(sample_indices)\n",
    "            # Create cluster and add the cluster to a list\n",
    "            cluster = Cluster()\n",
    "            cluster.id = clust.id\n",
    "            cluster.previous_id = clust.previous_id\n",
    "            cluster.next_id = clust.next_id\n",
    "            cluster.proto = clust.proto\n",
    "            cluster.sample_indices = sample_indices\n",
    "            pruned_clusters.append(cluster)\n",
    "    for i in pruned_clusters:\n",
    "        prototype_ids.append(i.proto[1])\n",
    "    return pruned_clusters, prototype_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters(dataset: np.ndarray, sample_index: list, clusters: list):\n",
    "    \"\"\"\n",
    "    This function assigns each row in the dataset to a cluster\n",
    "    \n",
    "    Args:\n",
    "        dataset (ndarray): dataset to assign clusters to\n",
    "        sample_index (integer list): a list of indices of the sample dataset used in the prototypical clustering\n",
    "        clusters (Cluster list): A list of pruned clusters where each cluster has a unique set of sample indices\n",
    "    \n",
    "    Return:\n",
    "        assignments (DataFrame): contains information about the cluster assigned to each row in the dataset\n",
    "            columns: cluster_id, prototype_id, assigned_id\n",
    "                cluster_id: the id of the Cluster object\n",
    "                prototype_id: the row index of the prototype (center point in a cluster)\n",
    "                assigned_id: assign a numerical id beginning at 0 ranging to the number of clusters\n",
    "    \"\"\"\n",
    "    assignments = pd.DataFrame(columns=[\"cluster_id\", \"prototype_id\", \"assigned_id\"])\n",
    "    N_dataset = dataset.shape[0]\n",
    "    p_dataset = dataset.shape[1]\n",
    "    N_clusters = len(clusters)\n",
    "\n",
    "    # Create a dictionary of cluster ids where the (key, value) is (cluster id, assigned id)\n",
    "    cluster_ids = dict()\n",
    "    for i in range(len(clusters)):\n",
    "        cluster_ids[clusters[i].id] = i\n",
    "\n",
    "    # Create arrays for each column\n",
    "    cluster_id = np.zeros(N_dataset, dtype=np.int64)\n",
    "    prototype_id = np.zeros(N_dataset, dtype=np.int64)\n",
    "    assigned_id = np.zeros(N_dataset, dtype=np.int64)\n",
    "\n",
    "    # Populate array with predetermined cluster assignments by prototypical clustering\n",
    "    for clust in clusters:\n",
    "        for index in clust.sample_indices:\n",
    "            cluster_id[sample_index[index]] = clust.id\n",
    "            prototype_id[sample_index[index]] = clust.proto[1]\n",
    "            assigned_id[sample_index[index]] = cluster_ids[clust.id]\n",
    "\n",
    "    # Create an array of each cluster's prototype row in the dataset\n",
    "    prototypes = np.zeros((N_clusters, p_dataset))\n",
    "    for i in range(N_clusters):\n",
    "        prototype = dataset[sample_index[clusters[i].proto[1]], :]\n",
    "        prototypes[i, :] = prototype\n",
    "\n",
    "    # Determine the Euclidean distances between the prototypes and the unselected dataset\n",
    "    distances = np.zeros((N_dataset, N_clusters))\n",
    "    for i in range(N_clusters):\n",
    "        # Repeat prototype row for entire length of dataset\n",
    "        temp_distances = np.repeat(np.reshape(prototypes[i], (-1, p_dataset)), repeats=N_dataset, axis=0)\n",
    "        # Determine the Euclidean distances e.g. (x - x1)^2\n",
    "        temp_distances = np.power((temp_distances - dataset), 2)\n",
    "        # Sum up the distances in a row of the distance matrix\n",
    "        distances[:, i] = np.sum(temp_distances, axis=1)\n",
    "\n",
    "    # Assign a prototype with the minumum distance to every row in the dataset\n",
    "    minimum_distance = np.argmin(distances, axis=1)\n",
    "    for i in range(N_dataset):\n",
    "        if cluster_id[i] == 0 and prototype_id[i] == 0 and assigned_id[i] == 0:\n",
    "            cluster_id[i] = clusters[minimum_distance[i]].id\n",
    "            prototype_id[i] = clusters[minimum_distance[i]].proto[1]\n",
    "            assigned_id[i] = cluster_ids[clusters[minimum_distance[i]].id]\n",
    "\n",
    "    # Populate assignments datafram\n",
    "    assignments[\"cluster_id\"] = cluster_id.tolist()\n",
    "    assignments[\"prototype_id\"] = prototype_id.tolist()\n",
    "    assignments[\"assigned_id\"] = assigned_id.tolist()\n",
    "    return assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_testing_sets(dataset: pd.DataFrame, dist_matrix: np.ndarray, prototype_ids: list, assignments: pd.DataFrame, test_size: float):\n",
    "    \"\"\"\n",
    "    This function assigns clusters to the training and testing set\n",
    "    \n",
    "    Args:\n",
    "        dataset (DataFrame): dataset to assign training and testing sets\n",
    "        dist_matrix (ndarray): This is an numpy array containing the distances between all samples.\n",
    "        prototype_ids (integer list): A list of row indices of the prototype (center point in a cluster)\n",
    "        assignments (DataFrame): contains information about the cluster assigned to each row in the dataset\n",
    "            columns: cluster_id, prototype_id, assigned_id\n",
    "                cluster_id: the id of the Cluster object\n",
    "                prototype_id: the row index of the prototype (center point in a cluster)\n",
    "                assigned_id: assign a numerical id beginning at 0 ranging to the number of clusters\n",
    "        test_size (float): a precentage of the testing set size (decimal form)\n",
    "    \n",
    "    Return:\n",
    "        training_set (DataFrame): A DataFrame of the training set\n",
    "        testing_set (DataFrame): A DataFrame of the testing set\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    training_set = pd.DataFrame(columns=dataset.columns)\n",
    "    testing_set = pd.DataFrame(columns=dataset.columns)\n",
    "    training_set_clusters = list()\n",
    "    testing_set_clusters = list()\n",
    "    testing_percent = 0.0\n",
    "\n",
    "    # Determine the cluster that is furthest distance away from all other clusters\n",
    "    iterations = 0\n",
    "    while not testing_set_clusters:\n",
    "        subset_prototype_ids = list(set(prototype_ids) - set(training_set_clusters))\n",
    "        prototype_dist = dist_matrix[np.ix_(subset_prototype_ids, subset_prototype_ids)]\n",
    "        max_distance = np.argmax(prototype_dist, axis=1)\n",
    "        max_dist_id = prototype_ids[mode(max_distance)]\n",
    "\n",
    "        # Get the cluster from the dataset\n",
    "        max_dist_assignments = assignments[assignments['prototype_id'] == max_dist_id]\n",
    "        max_dist_indices = max_dist_assignments.index.tolist()\n",
    "        cluster = dataset.iloc[max_dist_indices, :]\n",
    "        cluster_percentage = float(len(cluster)) / len(dataset)\n",
    "\n",
    "        # Find a cluster whose size is less than the testing size\n",
    "        if cluster_percentage < test_size:\n",
    "            testing_set = testing_set.append(cluster)\n",
    "            testing_set_clusters.append(max_dist_id)\n",
    "            testing_percent = float(len(testing_set)) / len(dataset)\n",
    "        else:\n",
    "            training_set = training_set.append(cluster)\n",
    "            prototype_ids.remove(max_dist_id)\n",
    "            training_set_clusters.append(max_dist_id)\n",
    "\n",
    "        # Catch if infinite loop\n",
    "        iterations += 1\n",
    "        if iterations > len(prototype_ids):\n",
    "            break\n",
    "\n",
    "    # Determine the distances between prototypes and initialize previously selected prototypes to extremely high number (for min)\n",
    "    assigned_indices = list()\n",
    "\n",
    "    for index, val in enumerate(prototype_ids):\n",
    "        for clust in training_set_clusters:\n",
    "            if val == clust:\n",
    "                assigned_indices.append(index)\n",
    "        for clust in testing_set_clusters:\n",
    "            if val == clust:\n",
    "                assigned_indices.append(index)\n",
    "\n",
    "    test_prototype_dist = dist_matrix[np.ix_(assigned_indices, prototype_ids)]\n",
    "    for i in range(len(assigned_indices)):\n",
    "            test_prototype_dist[i, assigned_indices] = 1e10\n",
    "\n",
    "    # Add clusters to the testing set until bigger than the testing size\n",
    "    iterations = 0\n",
    "    while testing_percent < test_size:\n",
    "        # Find prototype with the minimum distance from other prototypes in the testing set\n",
    "        minimum_index = np.unravel_index(np.argmin(test_prototype_dist, axis=None), test_prototype_dist.shape)\n",
    "\n",
    "        # Add the prototype cluster that is the closest distance to the previously selected prototypes to the testing set\n",
    "        min_dist_assignments = assignments[assignments['assigned_id'] == minimum_index[1]]\n",
    "        min_dist_indices = min_dist_assignments.index.tolist()\n",
    "        cluster = dataset.iloc[min_dist_indices, :]\n",
    "        testing_set = testing_set.append(cluster)\n",
    "        testing_percent = float(len(testing_set)) / len(dataset)\n",
    "        \n",
    "        # Update the distance matrix and list of selected prototypes\n",
    "        assigned_indices.append(minimum_index[1])\n",
    "        test_prototype_dist = dist_matrix[np.ix_(assigned_indices, prototype_ids)]\n",
    "        for i in range(len(assigned_indices)):\n",
    "            test_prototype_dist[i, assigned_indices] = 1e10\n",
    "        \n",
    "        # Catch if infinite loop\n",
    "        iterations += 1\n",
    "        if iterations > len(prototype_ids):\n",
    "            break\n",
    "    \n",
    "    # Assign clusters to training set\n",
    "    test_index = testing_set.index.tolist()\n",
    "    dataset_index = dataset.index.tolist()\n",
    "    training_indices = list(set(dataset_index) - set(test_index))\n",
    "    training_set = dataset.iloc[training_indices, :]\n",
    "    \n",
    "    return training_set, testing_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Get environment variables\n",
    "    start = time.time()\n",
    "    split = json.loads(os.getenv(\"SPLIT\"))\n",
    "    N = split[\"hierarchical_clustering\"][\"N\"]\n",
    "    max_clusters = split[\"hierarchical_clustering\"][\"max_clusters\"]\n",
    "    test_size = split[\"hierarchical_clustering\"][\"test_size\"]\n",
    "    \n",
    "    # Determines the euclidean distances of a dataset\n",
    "    split_file = data[\"DATASET\"]\n",
    "    dataset = pd.read_csv(split_file)\n",
    "    \n",
    "    # Filter dataset to contain only numeric columns\n",
    "    dataset_numeric = pd.read_csv(split_file)\n",
    "    for col in dataset.columns:\n",
    "        is_numeric = pd.api.types.is_numeric_dtype(dataset[col])\n",
    "        if not is_numeric:\n",
    "            dataset_numeric.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    dataset_array = dataset_numeric.to_numpy()\n",
    "    if dataset_numeric.shape[0] > N:\n",
    "        X = dataset_numeric.sample(n=N, random_state=1)\n",
    "    else:\n",
    "        X = dataset\n",
    "    X_index = X.index.tolist()\n",
    "    X = X.to_numpy()\n",
    "    dist_matrix = distance_matrix(X)\n",
    "    \n",
    "    # Perform hierarchical clustering by determining the minimax distance between two different clusters.\n",
    "    retired_clusters = nearest_neighbor(dist_matrix)\n",
    "    retired_clusters_copy = copy.deepcopy(retired_clusters)\n",
    "    \n",
    "    # Cut the tree to a specified number of clusters\n",
    "    clusters = get_clusters(retired_clusters_copy, max_clusters)\n",
    "\n",
    "    # Prunes clusters to contain unique sample indices for every cluster (no repeats)\n",
    "    pruned_clusters, prototype_ids = prune_clusters(clusters)\n",
    "    \n",
    "    # Assign each row in the dataset to a cluster\n",
    "    assignments = assign_clusters(dataset_array, X_index, pruned_clusters)\n",
    "    \n",
    "    # Assign clusters to the training and testing set\n",
    "    training_set, testing_set = get_training_testing_sets(dataset, dist_matrix, prototype_ids, assignments, test_size)\n",
    "\n",
    "    # Write the training and testing sets to files\n",
    "    training_set.to_csv('data/training_set.csv')\n",
    "    testing_set.to_csv('data/testing_set.csv')\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
